"""Data retrieval routes: trends, export, snapshots."""

import logging
import math
from datetime import datetime, timedelta

from flask import Blueprint, request, jsonify

from app.web import (
    require_auth,
    get_storage, get_config_manager, get_state,
    _localize_timestamps, _get_lang,
)
from app.gaming_index import compute_gaming_index

log = logging.getLogger("docsis.web")

data_bp = Blueprint("data_bp", __name__)


@data_bp.route("/api/trends")
@require_auth
def api_trends():
    """Return trend data for a date range.
    ?range=day|week|month&date=YYYY-MM-DD (date defaults to today)."""
    _storage = get_storage()
    if not _storage:
        return jsonify([])
    range_type = request.args.get("range", "day")
    date_str = request.args.get("date", datetime.now().strftime("%Y-%m-%d"))

    try:
        ref_date = datetime.strptime(date_str, "%Y-%m-%d")
    except ValueError:
        return jsonify({"error": "Invalid date format"}), 400

    if range_type == "day":
        data = _storage.get_intraday_data(date_str)
        _localize_timestamps(data)
        return jsonify(data)
    elif range_type == "week":
        start = (ref_date - timedelta(days=6)).strftime("%Y-%m-%d")
        data = _storage.get_summary_range(start, date_str)
        _localize_timestamps(data)
        return jsonify(data)
    elif range_type == "month":
        start = (ref_date - timedelta(days=29)).strftime("%Y-%m-%d")
        data = _storage.get_summary_range(start, date_str)
        _localize_timestamps(data)
        return jsonify(data)
    else:
        return jsonify({"error": "Invalid range (use day, week, month)"}), 400


@data_bp.route("/api/export")
@require_auth
def api_export():
    """Generate a structured markdown report for LLM analysis."""
    _storage = get_storage()
    _config_manager = get_config_manager()
    state = get_state()
    analysis = state.get("analysis")
    if not analysis:
        return jsonify({"error": "No data available"}), 404

    mode = request.args.get("mode", "full")
    if mode not in ("full", "update"):
        mode = "full"

    s = analysis["summary"]
    ds = analysis["ds_channels"]
    us = analysis["us_channels"]
    ts = state.get("last_update", "unknown")

    isp = _config_manager.get("isp_name", "") if _config_manager else ""
    conn = state.get("connection_info") or {}
    ds_mbps = conn.get("max_downstream_kbps", 0) // 1000 if conn else 0
    us_mbps = conn.get("max_upstream_kbps", 0) // 1000 if conn else 0

    lines = [
        "# DOCSight – DOCSIS Cable Connection Status Report",
        "",
        "## Context",
        "This is a status report from a DOCSIS cable modem generated by DOCSight.",
        "DOCSIS (Data Over Cable Service Interface Specification) is the standard for internet over coaxial cable.",
        "Analyze this data and provide insights about connection health, problematic channels, and recommendations.",
        f"- **Export Mode**: {'Full Context (48h)' if mode == 'full' else 'Update (6h)'}",
        "",
        "## Overview",
        f"- **ISP**: {isp}" if isp else None,
        f"- **Tariff**: {ds_mbps}/{us_mbps} Mbit/s (Down/Up)" if ds_mbps else None,
        f"- **Health**: {s.get('health', 'Unknown')}",
        f"- **Issues**: {', '.join(s.get('health_issues', []))}" if s.get('health_issues') else None,
        f"- **Timestamp**: {ts}",
        "",
        "## Summary",
        "| Metric | Value |",
        "|--------|-------|",
        f"| Downstream Channels | {s.get('ds_total', 0)} |",
        f"| DS Power (Min/Avg/Max) | {s.get('ds_power_min')} / {s.get('ds_power_avg')} / {s.get('ds_power_max')} dBmV |",
        f"| DS SNR (Min/Avg) | {s.get('ds_snr_min')} / {s.get('ds_snr_avg')} dB |",
        f"| DS Correctable Errors | {s.get('ds_correctable_errors', 0):,} |",
        f"| DS Uncorrectable Errors | {s.get('ds_uncorrectable_errors', 0):,} |",
        f"| Upstream Channels | {s.get('us_total', 0)} |",
        f"| US Power (Min/Avg/Max) | {s.get('us_power_min')} / {s.get('us_power_avg')} / {s.get('us_power_max')} dBmV |",
        "",
        "## Downstream Channels",
        "| Ch | Frequency | Power (dBmV) | SNR (dB) | Modulation | Corr. Errors | Uncorr. Errors | DOCSIS | Health |",
        "|----|-----------|-------------|----------|------------|-------------|---------------|--------|--------|",
    ]
    for ch in ds:
        lines.append(
            f"| {ch.get('channel_id','')} | {ch.get('frequency','')} | {ch.get('power','')} "
            f"| {ch.get('snr', '-')} | {ch.get('modulation','')} "
            f"| {ch.get('correctable_errors', 0):,} | {ch.get('uncorrectable_errors', 0):,} "
            f"| {ch.get('docsis_version','')} | {ch.get('health','')} |"
        )
    lines += [
        "",
        "## Upstream Channels",
        "| Ch | Frequency | Power (dBmV) | Modulation | Multiplex | DOCSIS | Health |",
        "|----|-----------|-------------|------------|-----------|--------|--------|",
    ]
    for ch in us:
        lines.append(
            f"| {ch.get('channel_id','')} | {ch.get('frequency','')} | {ch.get('power','')} "
            f"| {ch.get('modulation','')} | {ch.get('multiplex','')} "
            f"| {ch.get('docsis_version','')} | {ch.get('health','')} |"
        )

    # ── Historical context (events, speedtests, incidents) ──
    if _storage:
        if mode == "full":
            event_hours, speedtest_limit = 48, 10
        else:
            event_hours, speedtest_limit = 6, 3

        events = _storage.get_recent_events(hours=event_hours)
        if events:
            lines += [
                "",
                f"## Events (Last {event_hours}h)",
                "| Timestamp | Severity | Type | Message |",
                "|-----------|----------|------|---------|",
            ]
            for ev in events:
                lines.append(
                    f"| {ev['timestamp']} | {ev['severity']} | {ev['event_type']} | {ev['message']} |"
                )

        speedtests = []
        try:
            from app.modules.speedtest.storage import SpeedtestStorage
            _ss = SpeedtestStorage(_storage.db_path)
            speedtests = _ss.get_recent_speedtests(limit=speedtest_limit)
        except (ImportError, Exception):
            pass
        if speedtests:
            lines += [
                "",
                f"## Speedtest Results (Last {speedtest_limit})",
                "| Timestamp | Download | Upload | Ping | Jitter | Packet Loss |",
                "|-----------|----------|--------|------|--------|-------------|",
            ]
            for st in speedtests:
                lines.append(
                    f"| {st['timestamp']} | {st.get('download_human', '')} | {st.get('upload_human', '')} "
                    f"| {st.get('ping_ms', '-')} ms | {st.get('jitter_ms', '-')} ms "
                    f"| {st.get('packet_loss_pct', '-')}% |"
                )

        if mode == "full":
            try:
                from app.modules.journal.storage import JournalStorage
                _js = JournalStorage(_storage.db_path)
                entries = _js.get_active_entries()
            except (ImportError, Exception):
                entries = []
            if entries:
                lines += ["", "## Incident Journal"]
                for inc in entries:
                    lines.append(f"### [{inc['date']}] {inc['title']}")
                    if inc.get("description"):
                        lines.append(inc["description"])
                    lines.append("")

        # ── Cross-source correlation ──
        if speedtests:
            corr_lines = []
            for st in speedtests:
                snap = _storage.get_closest_snapshot(st["timestamp"])
                if snap:
                    ss = snap["summary"]
                    corr_lines.append(
                        f"| {st['timestamp'][:16]} | {st.get('download_human', '')} "
                        f"| {ss.get('health', '')} | {ss.get('ds_snr_min', '')} dB "
                        f"| {ss.get('ds_power_avg', '')} dBmV "
                        f"| {ss.get('ds_uncorrectable_errors', 0):,} |"
                    )
            if corr_lines:
                lines += [
                    "",
                    "## Cross-Source Correlation",
                    "Speedtest performance correlated with modem signal health at the time of each test.",
                    "",
                    "| Speedtest Time | Download | Modem Health | DS SNR Min | DS Power Avg | Uncorr. Errors |",
                    "|---------------|----------|-------------|------------|-------------|----------------|",
                ]
                lines.extend(corr_lines)

    # ── Dynamic reference values from active threshold profile ──
    from app import analyzer as _analyzer
    _thresh = _analyzer.get_thresholds()

    _meta = _analyzer._t().get("_meta", {})
    profile_name = _meta.get("operator", "Active Profile")

    lines += ["", f"## Reference Values ({profile_name})", ""]

    lines += [
        "### Downstream Power (dBmV)",
        "| Modulation | Good | Warning | Critical |",
        "|------------|------|---------|----------|",
    ]
    _ds = _thresh.get("downstream_power", {})
    for mod in sorted(k for k in _ds if not k.startswith("_")):
        t = _ds[mod]
        g = t.get("good", [0, 0])
        w = t.get("warning", [0, 0])
        c = t.get("critical", [0, 0])
        lines.append(f"| {mod} | {g[0]} to {g[1]} | {w[0]} to {w[1]} | < {c[0]} or > {c[1]} |")

    lines += [
        "",
        "### Upstream Power (dBmV)",
        "| Channel Type | Good | Warning | Critical |",
        "|-------------|------|---------|----------|",
    ]
    _us = _thresh.get("upstream_power", {})
    for key in sorted(k for k in _us if not k.startswith("_")):
        t = _us[key]
        g = t.get("good", [0, 0])
        w = t.get("warning", [0, 0])
        c = t.get("critical", [0, 0])
        lines.append(f"| {key} | {g[0]} to {g[1]} | {w[0]} to {w[1]} | < {c[0]} or > {c[1]} |")

    lines += [
        "",
        "### SNR / MER (dB, absolute)",
        "| Modulation | Good | Warning | Critical |",
        "|------------|------|---------|----------|",
    ]
    _snr = _thresh.get("snr", {})
    for mod in sorted(k for k in _snr if not k.startswith("_")):
        t = _snr[mod]
        lines.append(
            f"| {mod} "
            f"| >= {t.get('good_min', 0)} "
            f"| >= {t.get('warning_min', 0)} "
            f"| < {t.get('critical_min', 0)} |"
        )

    _err = _thresh.get("errors", {}).get("uncorrectable_pct")
    if _err:
        lines.append("")
        lines.append(f"**Uncorrectable Errors**: Warning >= {_err.get('warning', 1.0)}%, Critical >= {_err.get('critical', 3.0)}%")

    lines.append("")

    lines += [
        "## Questions",
        "Please analyze this data and provide:",
        "1. Overall connection health assessment",
        "2. Channels that need attention (with reasons)",
        "3. Error rate analysis and whether it indicates a problem",
        "4. Specific recommendations to improve connection quality",
    ]
    return jsonify({"text": "\n".join(l for l in lines if l is not None)})


@data_bp.route("/api/snapshots")
@require_auth
def api_snapshots():
    """Return list of available snapshot timestamps."""
    _storage = get_storage()
    if _storage:
        return jsonify(_storage.get_snapshot_list())
    return jsonify([])


@data_bp.route("/api/snapshots/<path:timestamp>")
@require_auth
def api_snapshot(timestamp):
    """Return full analysis for a specific snapshot."""
    _storage = get_storage()
    if not _storage:
        return jsonify({"error": "No storage"}), 500
    snap = _storage.get_snapshot(timestamp)
    if not snap:
        return jsonify({"error": "Snapshot not found"}), 404
    return jsonify(snap)
